# JavaEE 架构与应用小组作业

## 一、需求描述



1. 假设有 100 个主播在直播，分别不断收到观众打赏，观众数量量级约 30 万。

2. 打赏需记录明细并入账结算（按比例给主播分成，主播可随时提取分成）。

3. 支持观众查询个人标签画像：消费前 20% 为高消费人群，20%-80% 为中消费人群，后 20% 为低消费人群。

4. 支持查询某一主播下最高打赏的观众姓名 TOP10。

5. 支持查询每位主播的可领取打赏余额，主播提成比例可变化（变化前按原比例，变化后按新比例）。

6. 支持运营人员多维度查询：每小时打赏金额、主播、主播性别、观众性别四个维度组合查询。

## 二、非功能需求



1. 保证打赏信息不丢失、不重复（避免账目错误）。

2. 打赏处理需高效，依赖服务故障 / 网络不可达时仍需尽量为观众服务。

3. 每个服务记录 HTTP 请求参数，用 traceId 关联请求及后续处理日志。

4. 服务启动快速，无长时间预热。

## 三、假设



1. 每个服务多节点部署，节点可动态增减或故障。

2. 所有中间件（如 mysql、redis）无故障。

## 四、设计简述

### 服务架构

微服务架构图描述
一、架构核心组件
服务注册中心：Consul
微服务节点：
模拟服务：Node1、Node2
观众服务：Node1、Node2
财务服务：Node1、Node2
经营分析服务：Node1、Node2
数据库：
DB1
DB2
二、组件交互关系
1. 服务注册流程
所有微服务节点（模拟服务、观众服务、财务服务、经营分析服务的 Node1/Node2），均向 Consul 发起 “注册” 请求，完成服务注册。
2. 服务间调用
模拟服务 → 观众服务：
模拟服务 Node1 调用 观众服务 Node1
模拟服务 Node2 调用 观众服务 Node2
观众服务 → 财务服务：
观众服务 Node1 调用 财务服务 Node1
观众服务 Node2 调用 财务服务 Node2
财务服务 → 经营分析服务：
财务服务 Node1 调用 经营分析服务 Node1
财务服务 Node2 调用 经营分析服务 Node2
模拟服务 → 经营分析服务：
模拟服务 Node1、Node2 均调用 经营分析服务 Node2
3. 服务与数据库交互
观众服务（Node1、Node2）与 DB1 进行数据读写交互
财务服务（Node1、Node2）、经营分析服务（Node1、Node2）均与 DB2 进行数据读写交互

所有服务向 Consul 注册，包含模拟服务、观众服务、财务服务、经营分析服务，各服务均有多节点部署（Node1、Node2 等），对应数据库 DB1、DB2 等。

### 1. 模拟服务



* 功能：模拟用户打赏请求，每个节点每秒至少 500 次请求。

* 1.1 对外提供启动接口，调用时通过注册中心获取观众服务接口。

* 1.2 启动 n 个线程并行执行请求（初始 20 线程，可按需调整），日志打印请求次数（避免过多日志影响速度）。

* 1.3 两个模拟服务同时执行，生成 100 个主播和 30 万个观众的随机打赏数据。

### 2. 观众服务

#### 2.1 打赏请求接口



* 设计打赏记录表：id、主播 id、主播姓名、打赏人 id、打赏人姓名、打赏金额、打赏时间、traceId（traceId 列设为索引，防止重复提交）。

* 接口参数含上述字段，写入打赏记录表。

#### 2.2 观众个人标签查询接口



* 接口参数：观众 ID，通过 Feign 远程调用经营分析服务获取结果（需 2 秒内响应）。

* 超时 / 服务异常处理：降级返回友好提示，合理设计 Feign 超时、断路器超时和重试机制。

#### 2.3 主播 TOP10 打赏观众查询接口



* 复用作业 2 功能，验证模拟请求执行时功能正常。

* 多节点部署注意：通过配置指定节点或分布式锁（如 redis 锁）确保单一节点执行异步处理任务。

* 可使用 Spring Schedule 实现定时任务。

#### 2.4 打赏数据同步给财务服务



* 同步方式：观众服务推送或财务服务拉取（均通过 HTTP 接口）。

* 批量发送打赏记录表数据，主动方维护同步进度。

### 3. 财务服务

#### 3.1 打赏数据接收与结算



* 设计分成比例数据表：主播 ID、主播名称、分成比例。

* 设计结算表：主播 id、主播名称、结算金额、已提取金额。

* 结算方法：按分成比例更新结算表（定时执行或接口触发）。

* 从观众服务同步打赏记录（保持基本一致，允许轻微落后）。

#### 3.2 主播可提取金额查询与提取接口



* 查询接口：查询某主播结算金额、已提取金额、可提取金额。

* 提取接口：更新已提取金额，控制不超过结算金额。

### 4. 经营分析服务

#### 4.1 多维度查询接口



* 设计小时级别查询分析表：时间、主播性别、主播 id、主播名称、打赏人性别、打赏金额汇总。

* 接口支持按小时、性别、主播组合查询（如 18-22 点、男性观众、主播 A 的每小时打赏汇总）。

* 定时从打赏明细计算数据。

#### 4.2 观众画像查询接口



* 设计用户画像表：打赏人 id、打赏人姓名、打赏分位数、打赏金额、画像描述、更新时间。

* 定时计算观众打赏金额汇总，划分消费人群标签。

#### 4.3 数据更新规则



* 每轮更新需截至某条打赏明细，完成全部观众打赏金额更新后再计算分位数和画像。

* 设计进度表记录处理进度，每 2 小时执行一轮更新。

## 五、作业要求



1. 最多 4 名同学共同完成，需实现模拟服务所有功能。

2. 每个服务对应一个 Java Maven 项目。

3. 提交物包含：

* 项目源代码

* 表结构设计 SQL

* 每位参与者的分工说明

# 作业个人想法

1.使用consul管理服务注册与发现，使用spring cloud feign进行服务间调用
使用vue+elementUI进行后端数据展示和模拟服务控制（控制台）

2.需要redis缓存的服务需要内置redis客户端，mysql的访问可以增加redis缓存，还有其他例如直播间数据，标签数据，用户和观众等等也需要缓存，注意AOP切面和RDB读写

3.注意日志需要收集到后端管理页面展示，需要做好本地化和远程支持，日志需要包含traceId，方便问题排查

3.把基础实体细化为主播，直播间与观众，主播一一对应直播间，观众可以在多个直播间打赏主播，
主播和观众不只有性别，年龄，名称属性，还有ip的地理位置，标签属性，消费水平，
对于标签系统，需要为直播间和观众设计标签体系，标签可以是预设的，也可以是用户自定义的标签
预设标签可以是主播的风格，观众的兴趣爱好等，标签还需要维护和有关标签的关联度表，用于分析和推荐。

4.观众服务要支持创建，修改信息，查询信息，获取数据分析和画像等操作，观众要分为注册用户和游客用户

5.主播服务，需要支持主播注册，登录，修改信息，查询信息，获取数据分析等操作，主播还有1对应打赏的提现操作，需要支持修改分成比例操作，支持查询打赏数据
对于主播的直播间，要包含直播间状态（开播，关播，封禁等（要能解析直播间的直播画面数据和延迟，这里可以保留接口默认返回空画面）），直播间分类（游戏，娱乐，户外等），直播间标签（多标签支持），还有包含当场直播的收益，弹幕数据，观众数据，打赏数据，打赏top10等实时数据统计，还有支持踢出观众，禁言观众等操作，要能处理观众加入，离开，弹幕，打赏的信息


6.分析系统需要支持对主播个人和整体的分析，需要有可视化图表展示（热力图等等），分析系统还需要支持对观众的分析，能生成对应观众的标签画像，更新消费水平等，并且得做好定时分析的任务（且支持调整）

7.打赏系统需要支持高并发打赏请求，打赏请求需要异步处理，打赏请求需要保证不丢失不重复，可以使用消息队列进行解耦和异步处理，需要做好超时降级，重试和熔断处理，还要做好交易验证（这里不能接到实体支付接口，可以默认返回），

8.模拟服务比较复杂，要支持创建主播和直播间（可以选定标签），对直播间添加调整指定数额的随机观众（根据标签关联度和消费水平分层进行生成，还要注意分注册观众和游客观众，还要注意复用注册观众的数据），自动模拟观众的消息（加入离开，弹幕，打赏），每个直播间单独一个线程模拟，还要有模拟查看观众信息，主播信息，主播提现等等操作

9.后端控制台要支持模拟服务的可视化，支持日志查看，支持各个服务的状态查看（结合consul），支持数据报表查看，支持对每个直播间，每个用户（主播，用户）的数据查看查询，支持对分析服务返回的结果渲染成图表。

10.nginx服务器做反向代理，做负载均衡，做静态资源服务器，做安全防护（简单的防护，例如ip黑名单，简单的ddos防护等，模拟服务的服务访问除外）

11.最后数据库服务管理好两个数据库和各个表（主播，直播间，注册观众，标签（和关联度）||d打赏记录，提现记录等等），做好高并发和分布式支持和未知，错误返回。

12.做好安全相关，包括接口安全，数据安全，日志安全等，还有做好接口的幂等性设计，防止重复请求造成数据错误。