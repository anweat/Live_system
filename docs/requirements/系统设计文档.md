# 直播打赏系统 - 系统设计文档

## 1. 系统概述

### 1.1 项目背景

构建一个支持 100 个主播、30 万观众的高并发直播打赏系统，实现打赏记录、财务结算、数据分析和用户画像等功能。

### 1.2 核心目标

- 保证打赏信息不丢失、不重复
- 支持高并发打赏处理
- 提供多维度数据分析
- 实现观众消费画像标签系统
- 服务高可用与快速启动

## 2. 系统架构设计

### 2.1 架构风格

采用**微服务架构**，基于 Spring Cloud 生态构建分布式系统。

### 2.2 核心组件

- **服务注册中心**: Consul
- **服务间通信**: Spring Cloud OpenFeign
- **负载均衡**: Nginx + Ribbon
- **数据库**: MySQL (DB1、DB2 分库部署)
- **缓存**: Redis (数据缓存 + 分布式锁)
- **前端控制台**: Vue + ElementUI

### 2.3 服务模块划分

#### **Anchor Service (主播服务)**

- **职责**: 主播管理、直播间管理、打赏数据查询
- **核心功能**:
  - 主播注册、登录、信息管理
  - 直播间状态管理 (开播/关播/封禁)
  - 直播间实时数据统计 (观众数、收益、TOP10)
  - 直播间操作 (踢出观众、禁言)
  - 分成比例配置

#### **Audience Service (观众服务)**

- **职责**: 观众管理、打赏请求处理、数据同步
- **核心功能**:
  - 观众注册、游客支持、信息管理
  - 打赏请求接收与记录
  - 打赏数据批量同步至财务服务
  - 观众标签画像查询 (调用分析服务)
  - 主播 TOP10 打赏观众查询 (定时任务)

#### **Finance Service (财务服务)**

- **职责**: 财务结算、提现管理
- **核心功能**:
  - 打赏数据接收与结算
  - 分成比例管理 (支持动态调整)
  - 主播可提取金额查询
  - 主播提现处理 (幂等性设计)
  - 结算记录管理

#### **Data Analysis Service (数据分析服务)**

- **职责**: 多维度数据分析、用户画像生成
- **核心功能**:
  - 多维度组合查询 (时间、主播性别、观众性别)
  - 观众消费画像计算 (高/中/低消费人群)
  - 小时级打赏数据汇总
  - 定时数据分析任务 (每 2 小时)
  - 标签关联度分析

#### **Mock Service (模拟服务)**

- **职责**: 模拟打赏请求、测试数据生成
- **核心功能**:
  - 主播与直播间创建
  - 观众自动生成 (注册用户 + 游客)
  - 多线程并发打赏模拟 (每秒 500+请求)
  - 观众行为模拟 (加入/离开/弹幕/打赏)
  - 基于标签关联度的智能分配

#### **Backend Service (后端管理服务)**

- **职责**: 控制台管理、数据可视化
- **核心功能**:
  - 服务状态监控 (集成 Consul)
  - 日志收集与展示 (traceId 关联)
  - 数据报表与图表渲染
  - 直播间与用户数据查询
  - 模拟服务控制

#### **DB Service (数据库服务)**

- **职责**: 数据持久化管理
- **数据库分配**:
  - **DB1**: 观众服务数据 (观众信息、打赏记录)
  - **DB2**: 财务与分析数据 (结算、画像、分析报表)

#### **Nginx Service (反向代理服务)**

- **职责**: 负载均衡、静态资源、安全防护
- **核心功能**:
  - 服务反向代理与负载均衡
  - 静态资源服务 (前端页面)
  - 安全防护 (IP 黑名单、基础 DDoS 防护)

#### **Common Module (公共模块)**

- **职责**: 共享实体、工具类、日志追踪
- **核心组件**:
  - 实体类: 主播、直播间、观众、标签
  - 日志追踪: traceId 生成与传递
  - 工具类: 时间处理、加密、验证

## 3. 核心数据模型

### 3.1 基础实体

- **主播 (Anchor)**: ID、姓名、性别、年龄、IP 地理位置、标签、分成比例
- **直播间 (LiveRoom)**: ID、主播 ID、状态、分类、标签、当前观众数、实时收益
- **观众 (Audience)**: ID、姓名、性别、年龄、IP 地理位置、标签、消费水平、用户类型(注册/游客)
- **标签 (Tag)**: ID、标签名、标签类型、关联度权重

### 3.2 业务数据

- **打赏记录表**: 打赏 ID、主播 ID、观众 ID、金额、时间、traceId (索引)
- **结算表**: 主播 ID、结算金额、已提取金额、可提取金额
- **分成比例表**: 主播 ID、分成比例、生效时间
- **提现记录表**: 提现 ID、主播 ID、提现金额、状态、时间
- **用户画像表**: 观众 ID、打赏金额汇总、分位数、画像标签、更新时间
- **小时分析表**: 时间(小时)、主播 ID、主播性别、观众性别、打赏汇总
- **标签关联度表**: 标签 1、标签 2、关联度分数

## 4. 关键技术方案

### 4.1 高并发打赏处理

- **异步处理**: 使用线程池处理打赏请求，快速响应
- **幂等性设计**: traceId 作为唯一索引，防止重复提交
- **批量同步**: 打赏数据批量推送至财务服务，减少网络开销

### 4.2 数据一致性保证

- **分布式事务**: 使用分布式锁保证结算操作的原子性
- **数据同步机制**: 主动推送 + 断点续传，维护同步进度表
- **幂等性控制**: traceId + 状态机设计，防止重复处理

### 4.3 服务降级与容错

- **超时控制**: Feign 超时时间合理配置 (2 秒)
- **断路器**: Hystrix/Resilience4j 实现服务熔断
- **降级策略**: 远程调用失败时返回友好提示
- **重试机制**: 可重试的接口配置合理重试策略

### 4.4 缓存策略

- **热点数据缓存**: 直播间信息、主播信息、观众信息
- **查询优化**: TOP10 榜单、分析结果缓存
- **缓存更新**: 使用 Redis AOP 切面自动管理缓存失效
- **读写分离**: 主从模式提升查询性能

### 4.5 定时任务设计

- **Spring Schedule**: 定时任务统一管理
- **分布式锁**: Redis 分布式锁保证单节点执行
- **任务配置**: 支持动态调整执行频率
- **进度管理**: 维护进度表记录处理进度

### 4.6 标签系统

- **标签分类**: 不分类，支持多标签绑定，新标签创建
- **关联度计算**: 基于观众行为计算标签关联度，动态添加和更新标签关联度
- **智能推荐**: 根据标签匹配度分配观众

### 4.7 日志追踪

- **traceId 生成**: 网关层统一生成并传递
- **日志收集**: 本地日志 + 远程收集
- **日志展示**: 控制台支持按 traceId 查询完整链路

### 4.8 安全设计

- **接口安全**: Token 认证 + 签名验证
- **数据安全**: 敏感数据加密存储
- **防重放**: 时间戳 + nonce 机制
- **网络安全**: Nginx IP 黑名单、限流防护

## 5. 部署架构

### 5.1 服务部署

- 每个服务多节点部署 (至少 2 个节点)
- 支持节点动态扩缩容
- Consul 实现服务自动注册与发现

### 5.2 数据库部署

- DB1: 观众服务专用
- DB2: 财务、分析服务共享
- 支持主从复制、读写分离

### 5.3 负载均衡

- Nginx 反向代理
- Ribbon 客户端负载均衡
- 健康检查与故障转移

## 6. 关键流程

### 6.1 打赏流程

1. 观众发起打赏请求 (携带 traceId)
2. 观众服务验证并写入打赏记录表
3. 异步批量推送至财务服务
4. 财务服务按分成比例结算
5. 分析服务定时汇总统计

### 6.2 画像生成流程

1. 定时任务触发 (每 2 小时)
2. 读取打赏记录增量数据
3. 计算观众打赏金额汇总
4. 全量排序计算分位数
5. 更新用户画像表

### 6.3 提现流程

1. 主播发起提现请求
2. 财务服务验证可提取金额
3. 幂等性检查 (防止重复提现)
4. 更新已提取金额
5. 记录提现记录

## 7. 性能指标

- **打赏处理**: 单节点 >500 TPS，双节点 >1000 TPS
- **接口响应**: 查询类接口 <2s，打赏接口 <500ms
- **数据一致性**: 打赏数据最终一致性 <5 分钟
- **服务可用性**: 99.9% (支持故障转移)
- **启动时间**: 服务启动 <30s (无长时间预热)

## 8. 监控与运维

### 8.1 监控指标

- 服务健康状态 (Consul 健康检查)
- 接口调用量、成功率、耗时
- 数据库连接池状态
- 缓存命中率
- 定时任务执行状态

### 8.2 日志管理

- 统一日志格式 (含 traceId)
- 日志分级: DEBUG/INFO/WARN/ERROR
- 日志集中收集与查询
- 异常日志告警

### 8.3 问题排查

- traceId 链路追踪
- 接口调用链可视化
- 性能分析与瓶颈定位

## 9. 扩展性设计

- **服务水平扩展**: 通过增加节点提升处理能力
- **数据库分库分表**: 按主播 ID 或观众 ID 进行分片
- **消息队列引入**: 进一步解耦与异步处理 (预留)
- **微服务拆分**: 按业务域进一步细化服务粒度

## 10. 技术栈总结

| 层次     | 技术选型                   |
| -------- | -------------------------- |
| 前端     | Vue 3 + ElementUI          |
| 后端框架 | Spring Boot + Spring Cloud |
| 服务注册 | Consul                     |
| 服务通信 | OpenFeign                  |
| 负载均衡 | Nginx + Ribbon             |
| 数据库   | MySQL 8.0                  |
| 缓存     | Redis                      |
| 服务保护 | Hystrix / Resilience4j     |
| 日志追踪 | 自定义 traceId 机制        |
| 定时任务 | Spring Schedule            |
| 分布式锁 | Redis                      |
| 容器化   | Docker + Kubernetes (可选) |

---

**文档版本**: v1.0  
**编制日期**: 2026-01-01
